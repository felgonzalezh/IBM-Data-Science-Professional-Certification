{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different tools for each task a Data Scientist must cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Managment: \n",
    "Process of persisting and retrieving data.\n",
    "* Relational databases: MySQL and PostgreSQL.\n",
    "* NoSQL databases: MongoDB, Apache CouchDB, and Apache Cassandra.\n",
    "* file-based tools: Hadoop HDFS \n",
    "* Cloud File systems: Ceph\n",
    "* Storing: Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Integration and Transformation: ETL.\n",
    "* Apache AirFlow (created by AirBNB)\n",
    "* KubeFlow: Allows to execute data science pipelines on top of Kubernetes.\n",
    "* Apache Kafka (created from LinkedIn)\n",
    "* Apache Nifi: provides good visual editor\n",
    "* Apache SparkSQL: allows to use ANSI SQL and scales up to compute clusters of 1000s of nodes.\n",
    "* NodeRED: provides good visual editor. Little resources consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Visualization: \n",
    "Used for initial data exploration and final deliverable.\n",
    "* Hue can create visualizations from SQL queries\n",
    "* Kibana: (limited to Elasticsearch) It is a data exploration and visualization web application.\n",
    "* Apache Superset: a data exploration and visualization web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Building: Creating ML or DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Deployment: \n",
    "Process of deploy model to third-party applications.\n",
    "* Apache PredictionIO only supports Apache Spark ML models for deployment.\n",
    "* Seldon supports TensorFlow, Apache SparkML, R, and scikit-learn. It can run on top of Kubernetes and Redhat OpenShift. \n",
    "* MLeap deploys SparkML models. \n",
    "* TensorFlow can serve any of its models using the TensorFlow service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Monitoring and Assesment: \n",
    "Ensures continuous performance quality of the model.\n",
    "\n",
    "6.1 For accuracy\n",
    "* ModelDB is a ML metadatabase where you can find information about models. It supports Apache Spark ML Pipelines and scikit-learn. \n",
    "* Prometheus is used for ML model monitoring.\n",
    "6.2 For Bias against\n",
    "* The IBM AI Fairness 360. It detects and mitigates against bias in ML models. \n",
    "* The IBM Adversarial Robustness 360 Toolbox can be used to detect vulnerability to adversarial attacks and help make the model more robust.\n",
    "* The IBM AI Explainability 360 Toolkit makes the ML process more understandable. It can be used to build simple ML models, showing how input variables impact on the trainning and consequently in the final decision of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Code Asset Management: \n",
    "Used to control versions of the development in a team-work framework. \n",
    "* Git\n",
    "* Github\n",
    "* GitLab\n",
    "* BitBucket\n",
    "\n",
    "### 8 Data Asset Management: \n",
    "Used to control versions of data. It also supports replication, backup, and access right management\n",
    "* Apache Atlas\n",
    "* ODPi Egeria\n",
    "* Kylo \n",
    "\n",
    "### 9 Integrated Development Enviroments (IDE): \n",
    "Tools used to implement, execute, test, and deploy their work.\n",
    "* Jupyter supports more than a hundred different programming languages through “kernels”. Kernels shouldn’t be confused with operating system kernels. Jupyter kernels are encapsulating the different interactive interpreters for the different programming languages. It is usefull since it can be used for documentation, coding, display output from the code, shell commands, and visualizations into a single document. \n",
    "* Apache Zeppelin is inspired by Jupyter Notebooks. Plotting doesn’t require coding. \n",
    "* RStudio: It exclusively runs R and all associated R libraries. Python development is possible. RStudio unifies programming, execution, debugging, remote data access, data exploration, and visualization into a single tool. \n",
    "* Spyder\n",
    "\n",
    "### 10 Execution Enviroments: \n",
    "where data preprocessing, model training, and deployment take place \n",
    "* Apache Spark. It is a cluster-computing framework with linear scalability. This means, if you double the number of servers in a cluster, you’ll also roughly double its performance. \n",
    "* Apache Flink is a stream processing image, with its main focus on processing real-time data streams. \n",
    "* Ray has a clear focus on large-scale deep learning model training. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Integrated Development Enviroments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python libraries: \n",
    "\n",
    "* Pandas offers data structures and tools for effective data cleaning, manipulation, and analysis. Pandas is actually built on top of NumPy Data visualization methods\n",
    "* NumPy libraries are based on arrays, enabling you to apply mathematical functions to these arrays. \n",
    "* Matplotlib is the for data visualization\n",
    "* Seaborn is the for data visualization and it is based on matplotlib. heat maps, time series, and violin plots. \n",
    "* the Scikit-learn is a library for ML which contains tools for statistical modeling, including regression, classification, clustering and others. It is built on NumPy, SciPy, and matplotlib\n",
    "* Keras enables you to build the standard DL model. Like Scikit-learn, the high-level interface enables you to build models quickly and simply. It can function using graphics processing units (GPU), but for many deep learning cases a lower-level environment is required. \n",
    "* TensorFlow is a low-level framework used in large scale production of DL models. It’s designed for production but can be unwieldy for experimentation. \n",
    "* Pytorch is used for experimentation, making it simple for researchers to test their ideas\n",
    "* Apache Spark is a general-purpose cluster-computing framework that enables you to process data using compute clusters. This means that you process data in parallel, using multiple computers simultaneously. The Spark library has similar functionality as Pandas Numpy Scikit-learn. Apache Spark data processing jobs can use Python R Scala, or SQL There are many libraries for Scala, which is predominately used in data engineering but is also sometimes used in data science. \n",
    "\n",
    "\n",
    "## R \n",
    "It has built-in functionality for machine learning and data visualization\n",
    "* ggplot2 is for data visualization \n",
    "* There are libraries to interface with Keras and TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
