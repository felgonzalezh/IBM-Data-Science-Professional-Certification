{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To evaluate the performace of our model\n",
    " We need to split our data in two: \n",
    " \n",
    " * 75% of the data: training data \n",
    " * 25% of the data: testing data\n",
    " \n",
    " Testing data will help us to get an idea how the model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function train_test_split \n",
    "Function train_test_split (from the package model_selection of the library sklearn) splits randomly the data in two: training data and testin data. Its arguments are:\n",
    "* independent variables, \n",
    "* target variable, \n",
    "* test_size (percentage of the size for the testing set), and \n",
    "* random_state is the seed for randomly split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Performance\n",
    "\n",
    "Generalization error is a mesure of how well our data does to predict unseen data.\n",
    "\n",
    "Consider the following situations:\n",
    "- 90% of data is used for training and 10% for testing. Here, of course we are going to get a good fit for the training data. However, this is not necessarly true for the testing data, which means we are not going to get a good fit for the testing data. If we repeat the process using different sets of training data, each time we are going to get a different results. We get a good approximation of the True generalization performance but the precision is poor.\n",
    "- If we use fewer data point to train the model, we are going to get a good precision but less accuracy of the True generalization performance. \n",
    "\n",
    "To overcome this problem, we use CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation.\n",
    "\n",
    "Here the data is split into k equal groups, usually 4 folds. Three of them are used for training and the remaining for testing. This is repeated until each partition is used for both, training and testing. At he end we use the average to estimate the out-of-sample error. \n",
    "\n",
    "The evaluation metrics depends on the model. \n",
    "\n",
    "To perform the cross-validation we use the function cross_val_score() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funtion cross_val_score()\n",
    "\n",
    "Function train_test_split (from the package model_selection of the library sklearn) uses as arguments:\n",
    "* Type of model (linear regression)\n",
    "* x_data  (the predictive variable data), and \n",
    "* y_data  (the target variable data)\n",
    "* cv (number of partitions)\n",
    "The function returns an array, which lenght = cv, of scores. We can average the results to estimate out-of-sample using the mean method of NumPy. In the case of a linear regression, score will show us the r-squared\n",
    "\n",
    "### Function cross_val_predict()\n",
    "\n",
    "This function has the same arguments but its output is the prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting, underfitting and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection the proper polynomial order\n",
    "\n",
    "Underfitting: The model is to simple to fit the data.\n",
    "Overfitting:  Increasing the polynomial order to high might bring us  well tracking of the data points, but performs poorly at estimating the function. \n",
    "\n",
    "We select the order which minimize the MSE (Mean Squared Error). We can do a loop over each the polynomial order, model the data and find the R2 for each model, allowing us to fund out the proper order to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps us to prevent overfitting. \n",
    "\n",
    "When we have high polynomial orders, usually their terms in the expression have high coefficients. This probably means the we have overfitting.\n",
    "\n",
    "Ridge regression allow us to control the coefficient values. Alpha is the control paramenter, it must be chosen carefully because high values of alpha results in small coefficients, and we might probably have underfitting if they are close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "# RidgeModel = Ridge(alpha = 0.1)\n",
    "# RidgeModel.fit(X,y) train the model\n",
    "# Yhat = RidgeModel.predict(X) make a prediction\n",
    "# Yhay = RidgeModel.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the R-squared for different values of the alpha parameter to find out the proper one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow us to scan over multiple free parameters easily.\n",
    "\n",
    "Models usually depends on hyperparameters. Alpha is one example of an hyperparameter. Grid Search is a method that will helps to iterate automatically over these parameters using cross-validation. Grid Search calculates the R-suared or the MSE fro various parameter values. \n",
    "\n",
    "Here the full dataset is split in three: Training, Validation and Testing. The process is basically the same as before, except for Validation step where the hyperparameters values are optimized (which minimize MSE or maximize R-squared)\n",
    "\n",
    "Here we focus on Alpha and normalization parameters.\n",
    "\n",
    "The arguments for the gris search are:\n",
    "- The parameters \n",
    "- The object or model\n",
    "- The scoring method\n",
    "- Numer of folds (partitions)\n",
    "\n",
    "Some results include R2 values for each parameter value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter = [{\"alpha\":[1,10,100,1000]}]\n",
    "# RR = Ridge()  # Object or model\n",
    "# Grid1 = GridSearchVC(RR, parameter, cv=4) # Grid CV object\n",
    "# Grid1.fit(X,y)\n",
    "# Grid1.best_estimator_ # Find best parameter values \n",
    "# scores = Grid1.cv_results_ $ Get info like the mean squared\n",
    "# scores['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
