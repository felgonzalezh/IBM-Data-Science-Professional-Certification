{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "computers the ability to learn without being explicitly programmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regresion:  To predict a continous value (pricing)\n",
    "- Classification: To predict an item in a class/category in a case (benign/malign tumor)\n",
    "- Clustering: Finding the structure of data, summarization (custumer segmentation in banking)\n",
    "- Association: Associating: frequent occurent items/events (items bought together)\n",
    "- Anomaly detection: Discovering not usual cases. (Fraud in banking)\n",
    "- Sequence mining: predicting the next event (Streaming)\n",
    "- Dimension reduction: reducing the size of data\n",
    "- Recommendation systems: recommending items according to peoples preferences (books, videos, netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised\n",
    "\n",
    "Supervised how the computer learns: We teach the model so it can predict. We teach the model by training it with some data. (Regression and classification)\n",
    "\n",
    "- Unserpervised\n",
    "\n",
    "Fewer methods and less controled environment than supervised models.\n",
    "We dont supervise the model. The unsupervised algorith trains on the dataset, and draws conclusions. It helps to find data trends not evident to the eye. (Clustering, Dimension reduction).  \n",
    "* Dimension reduction: Reduce reduntant variables \n",
    "* Density estimation: to fins data structure \n",
    "* Clustering: Grouping data points that are somehow similar (Discovery structure, summarization, anomaly detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLR\n",
    "\n",
    "Minimize the Standast Error. Here is called residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model Evaluation\n",
    "\n",
    "- Training Accuracy:\n",
    "\n",
    "Is percentage of correct predictions that the model makes when using the test dataset. \n",
    "High accuracy could result in over-fitting  (overly trained by the data set, which may capture noise and produce a not generalized model)\n",
    "\n",
    "\n",
    "- Out-of-sample accuracy\n",
    "\n",
    "Out-of-sample accuracy is the percentage of correct predictions that the model makes on data that the model has not been trained on. How can be improved? Ussing Train/Test Split\n",
    "\n",
    "- k-fold validation\n",
    "\n",
    "Train the model several times: as an statistical ensamble in physics. So, we split the data into training and testing data sets and make the model (1-fold). Then, we split the data into a different traning and testing data sates and make the model (2-fold)... and so on, depending on the accuracy you want. (n-fold). Then \n",
    "the whole accuracy is the average of the k-fold's accurary.\n",
    "\n",
    "- Evaluation Metrics!\n",
    "\n",
    "MAE Mean Absolute Error: Mean of absoluted value of errors \n",
    "\n",
    "MSE Mean Squared Error (Variance)\n",
    "\n",
    "RMSE Root Mean Squared Error (Standart Deviation)\n",
    "\n",
    "RSE Relative Absolute Error: (residual sum of square) takes the total absolute error Sum(|yi - hat(y)|) and normalizes it by dividing by the total absolute error of the simple predictor Sum(yi - bar(y)). \n",
    "\n",
    "R^2 = Models accuracy: How close your data values are to the fitted.\n",
    "R^2 = 1 - RSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR\n",
    "\n",
    "MLR is an extension of SLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approches:\n",
    "* Ordinary Least Squares: Regular Linear Algebra (Minimizing MSE). The problem is time-consuming for more tha 10k lines\n",
    "\n",
    "* Optimizing the values: Iteratevily minimize the error of the model:\n",
    "    - Gradient Descendet: Optimiza with random values for each coeficient, then caculates the error and traies to minimize it using the y'change of the coeficients in each iteration. Proper for a large dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Algorithms\n",
    "\n",
    "As usual, they can be expressed as a multiliple Linear Expression, and the least squares can be used.\n",
    "If R^2 is higher than 0.7, the model can be expresed as a linear rergession.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Trees\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- k-nearest neighbor\n",
    "- Logistic Regresion\n",
    "- Neural Networks\n",
    "- Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pick a value of k\n",
    "2. Calculate distance of unknown case from all cases\n",
    "3. Select the k-observations closest points (in training dataset) to the unknown case \n",
    "4. Predict the reponse of the unknown data point based on the most popular reponses of the k-nearest neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose the proper k value, the best practice is to calculate the accuracy for each k-value and choose the one with the higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard index (Jaccarg similarity coefficient)\n",
    "\n",
    "j(y,y_hat) = |y interection y_hat|/|y union y_hat|\n",
    "\n",
    "j(y,y_hat) = |y interection y_hat|/(|y|+|y_hat|+|y interection y_hat|),\n",
    "\n",
    "where \n",
    "\n",
    "y = actual values\n",
    "\n",
    "y_hat = predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score\n",
    "\n",
    "* Confusion Matrix\n",
    "\n",
    "-x axis = Predicted values (y_hat)\n",
    "\n",
    "-y axis = Actual values (y) rows\n",
    "\n",
    "TruePositive   FalseNegative\n",
    "\n",
    "FalsePositive  TrueNegatives\n",
    "\n",
    "Precision = is a measure of the accuracy, provided that a class label has been predicted = TP / (TP + FP)\n",
    "\n",
    "Recall = is the true positive rate = TP / (TP + FN)\n",
    "\n",
    "F1-Score = 2*(prc*rec)/(prc+rec)\n",
    "\n",
    "Avg Accuracy = Avg(F1-Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Loss\n",
    "\n",
    "A classifier could be as a result the probability of the class label (For example Logistic Regression). For each row:\n",
    "\n",
    "(y x log(y_hat)+ (1-y)xlog(1-y_hat))\n",
    "\n",
    "Idel log loss is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
