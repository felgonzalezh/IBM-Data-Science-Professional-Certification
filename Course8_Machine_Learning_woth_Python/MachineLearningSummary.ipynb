{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "\n",
    "computers the ability to learn without being explicitly programmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regresion:  To predict a continous value (pricing)\n",
    "- Classification: To predict an item in a class/category in a case (benign/malign tumor)\n",
    "- Clustering: Finding the structure of data, summarization (custumer segmentation in banking)\n",
    "- Association: Associating: frequent occurent items/events (items bought together)\n",
    "- Anomaly detection: Discovering not usual cases. (Fraud in banking)\n",
    "- Sequence mining: predicting the next event (Streaming)\n",
    "- Dimension reduction: reducing the size of data\n",
    "- Recommendation systems: recommending items according to peoples preferences (books, videos, netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised\n",
    "\n",
    "Supervised how the computer learns: We teach the model so it can predict. We teach the model by training it with some data. (Regression and classification)\n",
    "\n",
    "- Unserpervised\n",
    "\n",
    "Fewer methods and less controled environment than supervised models.\n",
    "We dont supervise the model. The unsupervised algorith trains on the dataset, and draws conclusions. It helps to find data trends not evident to the eye. (Clustering, Dimension reduction).  \n",
    "* Dimension reduction: Reduce reduntant variables \n",
    "* Density estimation: to fins data structure \n",
    "* Clustering: Grouping data points that are somehow similar (Discovery structure, summarization, anomaly detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLR\n",
    "\n",
    "Minimize the Standast Error. Here is called residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model Evaluation\n",
    "\n",
    "- Training Accuracy:\n",
    "\n",
    "Is percentage of correct predictions that the model makes when using the test dataset. \n",
    "High accuracy could result in over-fitting  (overly trained by the data set, which may capture noise and produce a not generalized model)\n",
    "\n",
    "\n",
    "- Out-of-sample accuracy\n",
    "\n",
    "Out-of-sample accuracy is the percentage of correct predictions that the model makes on data that the model has not been trained on. How can be improved? Ussing Train/Test Split\n",
    "\n",
    "- k-fold validation\n",
    "\n",
    "Train the model several times: as an statistical ensamble in physics. So, we split the data into training and testing data sets and make the model (1-fold). Then, we split the data into a different traning and testing data sates and make the model (2-fold)... and so on, depending on the accuracy you want. (n-fold). Then \n",
    "the whole accuracy is the average of the k-fold's accurary.\n",
    "\n",
    "- Evaluation Metrics!\n",
    "\n",
    "MAE Mean Absolute Error: Mean of absoluted value of errors \n",
    "\n",
    "MSE Mean Squared Error (Variance)\n",
    "\n",
    "RMSE Root Mean Squared Error (Standart Deviation)\n",
    "\n",
    "RSE Relative Absolute Error: (residual sum of square) takes the total absolute error Sum(|yi - hat(y)|) and normalizes it by dividing by the total absolute error of the simple predictor Sum(yi - bar(y)). \n",
    "\n",
    "R^2 = Models accuracy: How close your data values are to the fitted.\n",
    "R^2 = 1 - RSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR\n",
    "\n",
    "MLR is an extension of SLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approches:\n",
    "* Ordinary Least Squares: Regular Linear Algebra (Minimizing MSE). The problem is time-consuming for more tha 10k lines\n",
    "\n",
    "* Optimizing the values: Iteratevily minimize the error of the model:\n",
    "    - Gradient Descendet: Optimiza with random values for each coeficient, then caculates the error and traies to minimize it using the y'change of the coeficients in each iteration. Proper for a large dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Algorithms\n",
    "\n",
    "As usual, they can be expressed as a multiliple Linear Expression, and the least squares can be used.\n",
    "If R^2 is higher than 0.7, the model can be expresed as a linear rergession.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Trees\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- k-nearest neighbor\n",
    "- Logistic Regresion\n",
    "- Neural Networks\n",
    "- Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pick a value of k\n",
    "2. Calculate distance of unknown case from all cases\n",
    "3. Select the k-observations closest points (in training dataset) to the unknown case \n",
    "4. Predict the reponse of the unknown data point based on the most popular reponses of the k-nearest neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose the proper k value, the best practice is to calculate the accuracy for each k-value and choose the one with the higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard index (Jaccarg similarity coefficient)\n",
    "\n",
    "j(y,y_hat) = |y interection y_hat|/|y union y_hat|\n",
    "\n",
    "j(y,y_hat) = |y interection y_hat|/(|y|+|y_hat|+|y interection y_hat|),\n",
    "\n",
    "where \n",
    "\n",
    "y = actual values\n",
    "\n",
    "y_hat = predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score\n",
    "\n",
    "* Confusion Matrix\n",
    "\n",
    "-x axis = Predicted values (y_hat)\n",
    "\n",
    "-y axis = Actual values (y) rows\n",
    "\n",
    "TruePositive   FalseNegative\n",
    "\n",
    "FalsePositive  TrueNegatives\n",
    "\n",
    "Precision = is a measure of the accuracy, provided that a class label has been predicted = TP / (TP + FP)\n",
    "\n",
    "Recall = is the true positive rate = TP / (TP + FN)\n",
    "\n",
    "F1-Score = 2*(prc*rec)/(prc+rec)\n",
    "\n",
    "Avg Accuracy = Avg(F1-Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Loss\n",
    "\n",
    "A classifier could be as a result the probability of the class label (For example Logistic Regression). For each row:\n",
    "\n",
    "(y x log(y_hat)+ (1-y)xlog(1-y_hat))\n",
    "\n",
    "Idel log loss is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose an attribute from your data\n",
    "2. Calculate the significance of the attribute in splitting data\n",
    "3. Split data based on the value of the attribute\n",
    "4. Go step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Founding the proper attributs for the DT\n",
    "\n",
    "#### Entropy\n",
    "Measures the uncertanty (or randomness) in each leaf of the tree. The lower entropy the purer the node.\n",
    "\n",
    "E = - p(A)log p(A) - p(B)log p(B)\n",
    "\n",
    "where p is the ratio of a category (A or B) in the leaf.\n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "In order to decide wich attribute perfoms better, we check the information gain wihchi is the information that can increase the level of certainty after the splitting\n",
    "\n",
    "Information Gain = Entroy_before - Entropy_after_weighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "It returns a probability score between 0 and 1. It measures the probability of a case belonging to a specific class\n",
    "Its suitable if:\n",
    "1. Your target variable is binary.\n",
    "2. If you need probabilisic results.\n",
    "3. When you need a linear regression boundary\n",
    "\n",
    "\n",
    "In order to define if a case belong to a specific class, we measure its probability. But there will be a case with a p=0.5. If p > 0.5, then in belongs to the class A, if p < 0.5 then belongs to B. Its like a \"step function\". However, the step function is not ideal since it return the value of y_hat. Instead we use a sigmoide function, which returns the probability of belonging to a specific class. Then, now y_hat = SIGMA(theta \\times x). Sigmoide function also known as LOGISTIC function.\n",
    "\n",
    "SIGMA  = 1 / (1 + e^{theta x})\n",
    "\n",
    "It returns the probability of y = 1 given x. P(y = 1|x)\n",
    "or the probability of y = 0 given x: P(y = 0|x) = 1 - P(y = 1|x)\n",
    "\n",
    "Training process:\n",
    "1. Initialize theta with random values\n",
    "2. Calculate the model output y_hat = sigm(theta x)\n",
    "3. Compare y_hat with the actual value y, and comute the error (Cost Function).\n",
    "\n",
    "To communate the cost:\n",
    "Cost(y,y_hat) = (1/2)*(y_hat - y)^2\n",
    "J(theta) = (1/m)*SUM_{i-1}^{m}(Cost)\n",
    "\n",
    "But it's easier to find the minimal point wiht the following cost function:\n",
    "A good approximation is:\n",
    "\n",
    "if y = 1 => Cost(y,y_hat) =  - log(y_hat)\n",
    "if y = 0 => Cost(y,y_hat) = - log(1- y_hat)\n",
    "\n",
    "J(theta) = -(1/m)*SUM_{i-1}^{m}{y log(y_hat) + (1-y)log(1- y_hat)}\n",
    "\n",
    "4. Calculate de error for all cases (cost)\n",
    "5. Change theta to minimize the cost\n",
    "6. go back to step 2\n",
    "\n",
    "\n",
    "#### Optimization approach:\n",
    "How to change theta to get the minimal cost? There are several methods. The most common is gradient descent.\n",
    "\n",
    "Gradiant descent:\n",
    "Find the maximal gradient of the cost function at a point. The opposite direction of the gradient assures we are minimizing the error. we take a step and recalculate the gradient. \n",
    "\n",
    "theta_new = theta_old - \\eta * Grad(J)\n",
    "\n",
    "\n",
    "\n",
    "When to stop? Depends on the accuaracy of the model you wanna reach. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
